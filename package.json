{
  "name": "robots",
  "version": "0.1.0",
  "description": "Parse robots.txt files from a URL.",
  "private": true,
  "main": "app.js",
  "scripts": {
    "start": "node app.js",
    "configure": "node configure.js",
    "build": "gulp build",
    "dist": "gulp",
    "clean": "gulp clean"
  },
  "author": "Sam Saint-Pettersen <s.stpettersen+github@gmail.com>",
  "license": "MIT",
  "devDependencies": {
    "bootstrap": "^3.3.7",
    "gulp": "^3.9.1",
    "gulp-cssmin": "^0.2.0",
    "gulp-jsmin": "^0.1.5",
    "gulp-remove-empty-lines": "^0.1.0",
    "gulp-remove-line": "^1.0.4",
    "gulp-rimraf": "^0.2.1",
    "gulp-standard": "^10.0.0",
    "gulp-uglify": "^3.0.0",
    "headjs": "^1.0.3",
    "jquery": "*",
    "shelljs": "^0.7.8"
  },
  "dependencies": {
    "express": "^4.15.3",
    "express-handlebars": "^3.0.0",
    "fs-extra": "^4.0.0",
    "request": "^2.81.0"
  }
}
